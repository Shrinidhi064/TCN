{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9652282,"sourceType":"datasetVersion","datasetId":5895860}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-17T16:09:21.250265Z","iopub.execute_input":"2024-10-17T16:09:21.250724Z","iopub.status.idle":"2024-10-17T16:09:23.118398Z","shell.execute_reply.started":"2024-10-17T16:09:21.250677Z","shell.execute_reply":"2024-10-17T16:09:23.116906Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/futureprediction/updatedfuturepredictions.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install tqdm\n","metadata":{"execution":{"iopub.status.busy":"2024-10-17T16:28:59.173689Z","iopub.execute_input":"2024-10-17T16:28:59.174221Z","iopub.status.idle":"2024-10-17T16:29:16.542902Z","shell.execute_reply.started":"2024-10-17T16:28:59.174137Z","shell.execute_reply":"2024-10-17T16:29:16.541183Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (4.66.4)\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (mean_squared_error, mean_absolute_error, r2_score,\n                             mean_absolute_percentage_error, explained_variance_score,\n                             median_absolute_error, max_error, mean_squared_log_error,\n                             mean_poisson_deviance, mean_tweedie_deviance)\nfrom tqdm import tqdm\n\n# Load dataset\ndata = pd.read_csv('/kaggle/input/futureprediction/updatedfuturepredictions.csv')\n\n# Assuming there's a timestamp column named 'DateTime'\ndata['DateTime'] = pd.to_datetime(data['DateTime'])\ndata.set_index('DateTime', inplace=True)\n\n# Define time intervals based on the index\nconditions = [\n    (data.index.hour < 6),\n    (data.index.hour >= 6) & (data.index.hour < 12),\n    (data.index.hour >= 12) & (data.index.hour < 17),\n    (data.index.hour >= 17)\n]\ntimeframes = ['Night', 'Morning', 'Afternoon', 'Evening']\ndata['Timeframe'] = np.select(conditions, timeframes)\n\n# Split data into features and target\nX = data.drop(columns=['RZT', 'Timeframe'])\ny = data['RZT']\nX = X.select_dtypes(include=[np.number])\n\n# Proceed with train-test split and model training\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Gradient Boosting Regression\ngbr = GradientBoostingRegressor()\ngbr.fit(X_train, y_train)\n\n# Predict and evaluate Gradient Boosting Regression\ny_pred_gbr = gbr.predict(X_test)\n\n# Calculate and display multiple metrics\nprint(\"Gradient Boosting Regression Metrics:\")\nprint(\"1. RMSE:\", mean_squared_error(y_test, y_pred_gbr, squared=False))\nprint(\"2. MAE:\", mean_absolute_error(y_test, y_pred_gbr))\nprint(\"3. Median Absolute Error:\", median_absolute_error(y_test, y_pred_gbr))\nprint(\"4. R² Score:\", r2_score(y_test, y_pred_gbr))\nprint(\"5. MAPE:\", mean_absolute_percentage_error(y_test, y_pred_gbr))\nprint(\"6. Explained Variance Score:\", explained_variance_score(y_test, y_pred_gbr))\nprint(\"7. Max Error:\", max_error(y_test, y_pred_gbr))\nprint(\"8. Mean Squared Log Error:\", mean_squared_log_error(y_test, y_pred_gbr))\nprint(\"9. Mean Poisson Deviance:\", mean_poisson_deviance(y_test, y_pred_gbr))\nprint(\"10. Mean Tweedie Deviance:\", mean_tweedie_deviance(y_test, y_pred_gbr))\n\n# Generate optimized Gradient Boosting predictions for the entire dataset\ndata['RZT_Optimized_GBR'] = [gbr.predict(pd.DataFrame([row], columns=X.columns))[0] for row in tqdm(X.values, desc=\"Gradient Boosting Prediction\")]\n\n# Save to Excel\ndata.to_excel('RZT_Optimized_Predictions_GBR.xlsx', index=False)\nprint(\"Optimized Gradient Boosting predictions saved to 'RZT_Optimized_Predictions_GBR.xlsx'.\")\n\n# Calculate min and max for each timeframe\ntimeframe_stats_gbr = data.groupby('Timeframe')['RZT_Optimized_GBR'].agg(['min', 'max']).reset_index()\nprint(\"Optimized Gradient Boosting Predictions Min and Max for each Timeframe:\")\nprint(timeframe_stats_gbr)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-17T16:32:04.407360Z","iopub.execute_input":"2024-10-17T16:32:04.407801Z","iopub.status.idle":"2024-10-17T16:32:19.328529Z","shell.execute_reply.started":"2024-10-17T16:32:04.407759Z","shell.execute_reply":"2024-10-17T16:32:19.327264Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Gradient Boosting Regression Metrics:\n1. RMSE: 0.025084978885867516\n2. MAE: 0.01591421990713017\n3. Median Absolute Error: 0.00900909156517038\n4. R² Score: 0.9999131940684498\n5. MAPE: 0.0007866914620903658\n6. Explained Variance Score: 0.9999131941736996\n7. Max Error: 0.1822003873971667\n8. Mean Squared Log Error: 1.1985623469394176e-06\n9. Mean Poisson Deviance: 2.828961004332559e-05\n10. Mean Tweedie Deviance: 0.0006292561657044191\n","output_type":"stream"},{"name":"stderr","text":"Gradient Boosting Prediction: 100%|██████████| 7200/7200 [00:11<00:00, 624.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Optimized Gradient Boosting predictions saved to 'RZT_Optimized_Predictions_GBR.xlsx'.\nOptimized Gradient Boosting Predictions Min and Max for each Timeframe:\n   Timeframe        min        max\n0  Afternoon  17.381618  31.402678\n1    Evening  17.373185  31.862174\n2    Morning  17.341338  34.557887\n3      Night  17.341338  32.875898\n","output_type":"stream"}]}]}